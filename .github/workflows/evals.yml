name: Run Evals for portia-sdk-python

on:
  pull_request_target:
    branches:
      - main
    types:
      - opened
      - synchronize
      - labeled

permissions:
    contents: read
    pull-requests: read

jobs:
  check-label:
    runs-on: ubuntu-latest
    steps:
    - name: Check for safe_to_test label
      id: check_label
      uses: mheap/github-action-required-labels@v5
      with:
        mode: exactly
        count: 1
        labels: safe_to_test
    env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  evals:
    needs: check-label
    runs-on: ubuntu-latest-16core
    steps:
      - name: Checkout portia-sdk-python repo
        uses: actions/checkout@v4
        with:
          path: portia-sdk-python
          repository: portiaAI/portia-sdk-python
          token: ${{ secrets.PORTIA_GH_TOKEN }}

      - uses: actions/checkout@v3
      - uses: ./.github/actions/python-env-setup
        with:
          working-directory: ./platform/evals
          token: ${{ secrets.PORTIA_GH_TOKEN }}

      - uses: ./.github/actions/setup-docker-env
        with:
          env-file-content: ${{ secrets.BACKEND_ENV_FILE_CONTENT }}
          working-directory: ./platform

      - name: Verify server is up
        run: |
          echo "Checking Docker container status:"
          docker ps
          
          echo -e "\nChecking ports in use:"
          netstat -tulpn || ss -tulpn
          
          echo -e "\nAttempting to connect to server:"
          for i in {1..90}; do
            if curl -f http://0.0.0.0:8080/; then
              echo "Server is up!"
              exit 0
            fi
            echo "Waiting for server to start... (attempt $i/30)"
            sleep 2
          done
          echo "Server failed to start after 3 minutes"
          docker compose -f ./backend/compose.yaml logs
          exit 1

      - name: Check tool IDs
        working-directory: ./evals
        id: check_tool_ids
        env:
          PORTIA_API_KEY: ${{ secrets.PORTIA_EVAL_API_KEY }}
          PORTIA_API_ENDPOINT: http://0.0.0.0:8080
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          uv run python3 check_tool_ids.py

      - name: Evaluate Query Planner
        id: eval_query_planner
        uses: ./.github/actions/run-eval
        with:
          working-directory: ./platform
          eval_type: query-planner
          langsmith-api-key: ${{ secrets.LANGCHAIN_API_KEY }}
          langsmith-project: ${{ secrets.LANGCHAIN_PROJECT }}
          openai-api-key: ${{ secrets.OPENAI_API_KEY }}
          portia-api-key: ${{ secrets.PORTIA_EVAL_API_KEY }}

      - name: Evaluate Agent
        id: eval_agent_verifier
        uses: ./.github/actions/run-eval
        with:
          working-directory: ./platform
          eval_type: agent
          langsmith-api-key: ${{ secrets.LANGCHAIN_API_KEY }}
          langsmith-project: ${{ secrets.LANGCHAIN_PROJECT }}
          openai-api-key: ${{ secrets.OPENAI_API_KEY }}
          portia-api-key: ${{ secrets.PORTIA_EVAL_API_KEY }}

      - name: Summary results
        id: summary_results
        working-directory: ./platform/evals
        env:
          LANGCHAIN_TRACING_V2: "true"
          LANGCHAIN_ENDPOINT: "https://api.smith.langchain.com"
          LANGCHAIN_API_KEY: ${{ secrets.LANGCHAIN_API_KEY }}
          LANGCHAIN_PROJECT: ${{ secrets.LANGCHAIN_PROJECT }}
          PORTIA_API_KEY: ${{ secrets.PORTIA_EVAL_API_KEY }}
          PORTIA_API_ENDPOINT: http://0.0.0.0:8080
          AGENT_VERIFIER_EXPERIMENT_ID: ${{ steps.eval_agent_verifier.outputs.eval_name }}
          QUERY_PLANNER_EXPERIMENT_ID: ${{ steps.eval_query_planner.outputs.eval_name }}
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
          OPENWEATHERMAP_API_KEY: ${{ secrets.OPENWEATHERMAP_API_KEY }}
        run: |
          uv run jupyter nbconvert --to markdown --execute github_analysis.ipynb --output notebook_output.md --no-input
          # Removes style blocks that GitHub won't render properly
          sed -i '/<style[^>]*>/,/<\/style>/d' notebook_output.md
          cat notebook_output.md >> $GITHUB_STEP_SUMMARY

      - name: Check for evaluation failures
        run: |
          CONTAINS_THRESHOLD_BREACH=false

          # Check if the query planner has failing scores
          if [[ "${{ steps.eval_query_planner.outputs.has_failing_eval_planner_scores }}" == "true" ]]; then
            echo "Query planner eval failed or has breaches."
            echo "Breaches: ${{ steps.eval_query_planner.outputs.metric_breaches }}"
            CONTAINS_THRESHOLD_BREACH=true
          fi

          # Check if the verifier agent has failing scores
          if [[ "${{ steps.eval_agent_verifier.outputs.has_failing_eval_agent_scores }}" == "true" ]]; then
            echo "Agent eval (verifier) failed or has breaches."
            echo "Breaches: ${{ steps.eval_agent_verifier.outputs.metric_breaches }}"
            CONTAINS_THRESHOLD_BREACH=true
          fi

          # Exit with a non-zero status if any failures were detected
          if [[ "$CONTAINS_THRESHOLD_BREACH" == "true" ]]; then
            echo "One or more evaluations failed."
            exit 1
          fi
